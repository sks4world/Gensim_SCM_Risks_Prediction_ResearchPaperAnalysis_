import os
os.getcwd()
os.chdir('')

# Read data from a csv file, row by row
# Question: What happens if you drop utf-8 encoding?

import csv
import pandas as pd 
df = pd.read_csv('SCM_RiskPrediction.csv', delimiter="\t")
 
df.head(3)
df.shape
df

df.columns
df.loc[0]
text = []
textstring =''
for i in df.index:
    textstring = textstring + ' ' + df.loc[i]
    text.append(df.loc[i]) 

textstring
text
len(text)

sentence = ''.join(map(str, text))
sentence

import nltk
sentences = nltk.tokenize.sent_tokenize(sentence)
print(sentences)
len(sentences)


# From original corpus of words, filter our non-aplphabetic words

words1 = nltk.tokenize.word_tokenize(sentence)
print(words1[:200])

words1 = [w for w in words1 if w.isalpha()]
print(words1[:200])

words1 = [w.lower() for w in words1]
print(words1[:200])
len(words1)



# Question: Which corpus of words is better: that derived using NLTK or Python string? 

# Filter out stopwords

stop_words = nltk.corpus.stopwords.words('english')
#print(stop_words)
#len(stop_words)

nltk.FreqDist(words1)
words = [w for w in words1 if w not in stop_words]
#print(words[:100])
nltk.FreqDist(words)
len(words)

removewords = ['name', 'dtype', 'object', 'https', 'one','give', 'author', 'objecttext']
for word in list(words):
    if word in removewords:
        words.remove(word)
    if len(word) <3:
        words.remove(word)
                     
from nltk.stem.lancaster import LancasterStemmer
st = LancasterStemmer()

words_stemmed=[]
for word in list(words):
    words_stemmed.append(st.stem(word))
                     
from nltk.stem.porter import PorterStemmer
st = PorterStemmer()

words_stemmed_p=[]
for word in list(words):
    words_stemmed_p.append(st.stem(word))
                     
data_lem_vocab_freq_words = nltk.FreqDist(words_stemmed)
title_string="Top 5 words"
data_lem_vocab_freq_words.plot(25, cumulative=False, title=title_string)
                     
data_lem_vocab_freq_words = nltk.FreqDist(words_stemmed_p)
title_string="Top 5 words"
data_lem_vocab_freq_words.plot(25, cumulative=False, title=title_string)
                     
from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()

words_stemmed_lem=[]
for word in list(words):
    words_stemmed_lem.append(wordnet_lemmatizer.lemmatize(word))
                     
#Topic Modeling
#Credit for the following code: Thank you Michel Kana, PhD for the clean article at the link below on the topic 
#Topic Modeling Tutorial with Latent Dirichlet Allocation (LDA)
#https://towardsdatascience.com/topic-modeling-with-latent-dirichlet-allocation-by-example-3b22cd10c835
                     
from nltk.corpus import stopwords
en_stop_words = set(stopwords.words('english'))
list(en_stop_words)[:10]
                     
from sklearn.feature_extraction.text import CountVectorizer
from gensim.corpora import Dictionary
from gensim.models.ldamodel import LdaModel
from gensim.models import CoherenceModel
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from datetime import datetime
import nltk
nltk.download('stopwords')
import pandas as pd
import re
import math
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
                     
df_tweets = pd.read_csv('SCM_RiskPrediction.csv', delimiter="\t")
df_tweets.head()
                     
def clean_tweets(df=df_tweets, 
                 tweet_col='Text'):
    
    df_copy = df.copy()
    
    # drop rows with empty values
    df_copy.dropna(inplace=True)
   
    
    # lower the tweets
    df_copy['preprocessed_' + tweet_col] = df_copy[tweet_col].str.lower()
    
    # filter out stop words and URLs
    en_stop_words = set(stopwords.words('english'))
    extended_stop_words = en_stop_words | \
                        {
                            '&amp;', 'rt',                           
                            'th','co', 're', 've', 'kim', 'daca'
                        }
    url_re = '(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9]+\.[^\s]{2,}|www\.[a-zA-Z0-9]+\.[^\s]{2,})'        
    df_copy['preprocessed_' + tweet_col] = df_copy['preprocessed_' + tweet_col].apply(lambda row: ' '.join([word for word in row.split() if (not word in stop_words) and (not re.match(url_re, word))]))
    
    # tokenize the tweets
    tokenizer = RegexpTokenizer('[a-zA-Z]\w+\'?\w*')
    df_copy['tokenized_' + tweet_col] = df_copy['preprocessed_' + tweet_col].apply(lambda row: tokenizer.tokenize(row))
    
    return df_copy
  
df_tweets_clean = clean_tweets(df_tweets)
df_tweets_clean.head()
        
#Bag of words
def get_most_freq_words(str, n=None):
    vect = CountVectorizer().fit(str)
    bag_of_words = vect.transform(str)
    sum_words = bag_of_words.sum(axis=0) 
    freq = [(word, sum_words[0, idx]) for word, idx in vect.vocabulary_.items()]
    freq =sorted(freq, key = lambda x: x[1], reverse=True)
    return freq[:n]

get_most_freq_words([ word for tweet in df_tweets_clean.tokenized_Text for word in tweet],10)
        
#Finding number of topics
# build a dictionary where for each tweet, each word has its own id.
# We have 73 tweets and (n) words in the dictionary.
tweets_dictionary = Dictionary(df_tweets_clean.tokenized_Text)

# build the corpus i.e. vectors with the number of occurence of each word per tweet
tweets_corpus = [tweets_dictionary.doc2bow(tweet) for tweet in df_tweets_clean.tokenized_Text]

# compute coherence
tweets_coherence = []
for nb_topics in range(1,36):
    lda = LdaModel(tweets_corpus, num_topics = nb_topics, id2word = tweets_dictionary, passes=10)
    cohm = CoherenceModel(model=lda, corpus=tweets_corpus, dictionary=tweets_dictionary, coherence='u_mass')
    coh = cohm.get_coherence()
    tweets_coherence.append(coh)

# visualize coherence
plt.figure(figsize=(10,5))
plt.plot(range(1,36),tweets_coherence)
plt.xlabel("Number of Topics")
plt.ylabel("Coherence Score");
        
#Running LDA
k = 6
tweets_lda = LdaModel(tweets_corpus, num_topics = k, id2word = tweets_dictionary, passes=10)

def plot_top_words(lda=tweets_lda, nb_topics=k, nb_words=10):
    top_words = [[word for word,_ in lda.show_topic(topic_id, topn=50)] for topic_id in range(lda.num_topics)]
    top_betas = [[beta for _,beta in lda.show_topic(topic_id, topn=50)] for topic_id in range(lda.num_topics)]

    gs  = gridspec.GridSpec(round(math.sqrt(k))+1,round(math.sqrt(k))+1)
    gs.update(wspace=0.5, hspace=0.5)
    plt.figure(figsize=(20,15))
    for i in range(nb_topics):
        ax = plt.subplot(gs[i])
        plt.barh(range(nb_words), top_betas[i][:nb_words], align='center',color='blue', ecolor='black')
        ax.invert_yaxis()
        ax.set_yticks(range(nb_words))
        ax.set_yticklabels(top_words[i][:nb_words])
        plt.title("Topic "+str(i))

plot_top_words()
        
#Above two steps repeated for k = 10 and k = 15        
        
